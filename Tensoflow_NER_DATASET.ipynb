{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensoflow_NER_DATASET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeMCotU7StUayzBKtqnQMO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassanJoumaa/KAGGLE_NER_DATASET/blob/main/Tensoflow_NER_DATASET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzYoR6f8X40i"
      },
      "source": [
        "# **NER DATASET**\r\n",
        "This is a very clean dataset and is for anyone who wants to try his/her hand on the NER ( Named Entity recognition ) task of NLP.\r\n",
        "\r\n",
        "## ***1. Problem***\r\n",
        "\r\n",
        "We will use this dataset in order to train an NER model which will be able to successfully identify the different Tags in a sentence.\r\n",
        "\r\n",
        "## ***2. Data***\r\n",
        "\r\n",
        "The data we're using is from Kaggle's NER_dataset.\r\n",
        "\r\n",
        "https://www.kaggle.com/namanj27/ner-dataset\r\n",
        "\r\n",
        "## ***3. Evaluation***\r\n",
        "\r\n",
        "We will evaluate the model based on the accuracy metric.\r\n",
        "\r\n",
        "\r\n",
        "## ***4. Features***\r\n",
        "* The dataset with 1M x 4 dimensions contains columns = ['# Sentence', 'Word', 'POS', 'Tag'] and is grouped by #Sentence.\r\n",
        "\r\n",
        "**Columns**\r\n",
        "\r\n",
        "Word:\r\n",
        "This column contains English dictionary words form the sentence it is taken from.\r\n",
        "\r\n",
        "POS:\r\n",
        "Parts of speech tag\r\n",
        "\r\n",
        "Tag:\r\n",
        "Standard named entity recognition tags as follows:\r\n",
        "\r\n",
        "* ORGANIZATION - Georgia-Pacific Corp., WHO\r\n",
        "* PERSON - Eddy Bonte, President Obama\r\n",
        "* LOCATION - Murray River, Mount Everest\r\n",
        "* DATE - June, 2008-06-29\r\n",
        "* TIME - two fifty a m, 1:30 p.m.\r\n",
        "* MONEY - 175 million Canadian Dollars, GBP 10.40\r\n",
        "* PERCENT - twenty pct, 18.75 %\r\n",
        "* FACILITY - Washington Monument, Stonehenge\r\n",
        "* GPE - South East Asia, Midlothian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRpA6Ai93EXO"
      },
      "source": [
        "### **Import the Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zjijy1FYaOR"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJikThtb2z-f"
      },
      "source": [
        "### **Download the Data from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnNPuoRz3BAR"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqupgNGX_YR"
      },
      "source": [
        "# Adding the Username and Key from the Kaggle Token Folder\r\n",
        "os.environ['KAGGLE_USERNAME']=\"hassanjoumaa\"\r\n",
        "os.environ['KAGGLE_KEY']=\"d3077228d9aecdd27fd4f73a4fa4b31d\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGEWn_LGYLLx"
      },
      "source": [
        "# Downloading the Dataset from Kaggle\r\n",
        "!kaggle datasets download -d namanj27/ner-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-E1444HYR06"
      },
      "source": [
        "# Unziping the Folder\r\n",
        "!unzip ner-dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIzlQ2hct4T4"
      },
      "source": [
        "### **Get & Clean the Data** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC22dz1GloOJ"
      },
      "source": [
        "df = pd.read_csv(\"/content/ner-dataset.zip\", encoding='latin1')\r\n",
        "df.drop(\"POS\",axis=1, inplace=True)\r\n",
        "df.fillna(method=\"ffill\", inplace=True)\r\n",
        "df.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfiDLgNwuCyG"
      },
      "source": [
        "> ***Group the records by Sentence #***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU1BYRt5mowm"
      },
      "source": [
        "agg_fun = lambda s: [(w, t) for w, t in zip(s[\"Word\"],s['Tag'])]      \r\n",
        "grouped = df.groupby('Sentence #').apply(agg_fun)\r\n",
        "grouped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH2fk825uI5Y"
      },
      "source": [
        "### **Get the Sentences and their respective Tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhE67wS9nxFG"
      },
      "source": [
        "sentences = [[w[0] for w in s] for s in grouped]\r\n",
        "tags = [[t[1] for t in s] for s in grouped]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G2GoC7ZEkK4"
      },
      "source": [
        "print(sentences[0])\r\n",
        "print(tags[0])\r\n",
        "print(len(sentences[0]))\r\n",
        "print(len(tags[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSwrx_kowRr_"
      },
      "source": [
        "lengths = [len(s) for s in sentences]\r\n",
        "plt.hist(lengths, bins = 50)\r\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGVDn4PlyhTQ"
      },
      "source": [
        "> ***Plotting the lengths shows that it would be a good idea to take the max length for the input as 50.*** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47TUzl2CuQBT"
      },
      "source": [
        "### **Tokenizing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKZHI10RFCeJ"
      },
      "source": [
        "embedding_dim = 32\r\n",
        "max_length = 50\r\n",
        "trunc_type='post'\r\n",
        "padding_type='post'\r\n",
        "oov_tok = \"<OOV>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmPtLNeittII"
      },
      "source": [
        "#### ***Tokenizing the Sentences***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0IPjImscT-e"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token=oov_tok)\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "vocab_size = len(tokenizer.word_index)+1\r\n",
        "sequences = [[word_index[w.lower()] for w in s] for s in sentences]\r\n",
        "X = pad_sequences(sequences=sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3trIBL6mtxSG"
      },
      "source": [
        "#### ***Tokenizing the Labels***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FunqbthjfBg5"
      },
      "source": [
        "label_tokenizer = Tokenizer()\r\n",
        "label_tokenizer.fit_on_texts(tags)\r\n",
        "labels_word_index = label_tokenizer.word_index\r\n",
        "labels = [[labels_word_index[l.lower()] for l in t] for t in tags]\r\n",
        "padded_labels= np.array(pad_sequences(sequences=labels, maxlen=max_length, padding=padding_type, truncating=trunc_type, value=labels_word_index[\"o\"]))-1\r\n",
        "y = [to_categorical(l, num_classes=len(labels_word_index)) for l in padded_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHz4U0wVntyH"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(X_train), tf.constant(y_train)))\r\n",
        "train_dataset = train_dataset.batch(64)\r\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(X_val), tf.constant(y_val)))\r\n",
        "val_dataset = val_dataset.batch(64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMuhzlPjzQFu"
      },
      "source": [
        "### **Creating & training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HThRCkoBs8og"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "                             tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, recurrent_dropout=0.1)),\r\n",
        "                             tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(labels_word_index), activation='softmax'))\r\n",
        "])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSWxBvvzmfzt"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNKxcyA7mvb0"
      },
      "source": [
        "history = model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T_ZNnxJvhaL"
      },
      "source": [
        "### **Testing on our Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DeuQ6z5HCwZ"
      },
      "source": [
        "labels_index_word = dict([(value, key) for (key, value) in labels_word_index.items()])\r\n",
        "labels_index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lip5rjCtq8EK"
      },
      "source": [
        "def predict_on_sentence():\r\n",
        "  my_sentence = input(\"Enter your own sentence: \")\r\n",
        "  my_sequence = tokenizer.texts_to_sequences([my_sentence])\r\n",
        "  my_padded = pad_sequences(sequences=my_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\r\n",
        "  prediction = model.predict(np.array(my_padded))\r\n",
        "  p = np.argmax(prediction, axis=-1)+1\r\n",
        "  labeled_preds = [labels_index_word[label] for label in p[0]]\r\n",
        "  return labeled_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_OsFk4CE3G_"
      },
      "source": [
        "predict_on_sentence()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}